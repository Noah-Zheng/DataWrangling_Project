{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "[WeRateDogs](https://twitter.com/dog_rates) is the most popular twitter rating dogs.\n",
    "\n",
    "The WeRateDogs Twitter archive is provided for Udacity students to practice data wrangling skills. The archive includs information of tweet id, timestamp, text, rating_numerator, rating_demoninator etc. In addtion, a file of image prediction is provided and can be downloaded programmatically using `requests`. \n",
    "\n",
    "\n",
    "## Steps taken\n",
    "### Data Collecting\n",
    "The first part of data wrangling is to collect data.  \n",
    "The twitter_archive_enhanced.csv file is downloaded manually.\n",
    "\n",
    "The image_predictions.tsv file is downloaded programmatically using the `Requests` package. Using `requests.get()` to save the response to a variable and then use `write` function to store the content of the response to a file. \n",
    "\n",
    "The last data required is each tweet's JSON data. A twitter developer account is created to use Twitter's API to query each tweet in the archive file to extract JSON data used for later analysis. `tweepy` is used to conncet to twitter API.  A list of tweet_id is extracted from the archive file and is passed to `get_status()` function to get the data. At first, `tweet = api.get_status(t_id)` returned the tweet text rather than the JSON data required. The problem is solved by adding `._json` to the end and store the data in a variable named tweet. Then use `json` library and `open` command to store all the JSON data in a file. In this project, only information about retweet count and favorite count is extracted from the JSON data. Some other information such as timestamp of favorite would be usefull when exploring the trend of favorite during a period. \n",
    "\n",
    "After the dataframe including the tweet_id, retweet_count and favorite_count is created, the dataframe is save as a csv fiel using `dataframe.to_csv()` function.\n",
    "\n",
    "\n",
    "### Data Assessing\n",
    "The second part is to assess the data collected. \n",
    "All the data are imported using `panda.read_csv()` function. \n",
    "Visual assessment and programmactical assessment are used to identify several quality and tidiness issues. \n",
    "\n",
    "#### Visual Assessment\n",
    "Microsoft Excel and jupyter notebook were used to open files to do visual assessment. \n",
    "One tidiness issue that the dog stage should be structed using one column was easily noticed doing the visual assessment. One quality came up when some of the dog names are 'a', 'such' etc. As this information is not important in my analysis, it is ingonred. Some of the issues like incorrected extracted rating_numerating are pointed by the reviewer and checked in the excel.\n",
    "\n",
    "#### Programmatic Assessment\n",
    "The majority of the issues is found using programmatic assessment: the data type issues, the rating_denominator with value '0' issue, the completeness issue ect. Mainly used functions are `pandas.DataFrame.info()`, `pandas.DataFrame.describe()`, `duplicated()` etc. \n",
    "\n",
    "### Data Cleaning\n",
    "After documenting the issues comes the last part of data wrangling, cleaning the data. \n",
    "The tidiness issues are dealt with firstly and quality issues later. Afer the cleaning part is done, cleaned data is stored in twitter_archive_master.csv.\n",
    "\n",
    "One major problem is to melt the related columns in prediction table.  \n",
    "When solving the tidiness issue concerning the prejection table, I started with the method in the [link](https://github.com/pandas-dev/pandas/issues/17676), but faild with ValueError (setting an array element with a sequence). Cannot solve the problem. Tried to  melt the three groups one by one. One issue came up when merge these three melted table is the result table is much larger than the intented size which should be 6225. After look into the individual melted table, there are duplicates which will result in an increase in the rows when merge with another table with duplicates. Cannot solve this problem either. Find another method on [link](https://stackoverflow.com/questions/38862832/pandas-melt-several-groups-of-columns-into-multiple-target-columns-by-name ) which is `pd.wide_to_long`. At first, cannot figure out how stubnames and sep works. After changing the name as the example in the stakoverflow post, it worked. I realised that the stubnames should be related to the column names in the table. The result table is the result I wanted. However, one column indicating which prediction it is should be added in the table.  \n",
    "\n",
    "The major problem in solving the quality issues is regular expression. This pattern `'(\\d+\\.?\\d?/\\d+)'` was firstly used to extract the rating information from the text. After visually checking the text, `'(\\d+\\.?\\d?/10)'` is better. \n",
    "\n",
    "\n",
    "## Data Output\n",
    "After cleaning the data, the cleaned data is store in twitter_archive_master.csv file. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
